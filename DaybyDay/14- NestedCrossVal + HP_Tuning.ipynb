{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# import LogisitcRegression and RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit , KFold , train_test_split , cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# import gridsearchCV for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import MakeClassification for creating synthetic data\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Train Test Vs Cross-Validation Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first : train_test_split\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score : 0.8849999999999999\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#roc score is a metric for binary classifier , from 0 to 1 \n",
    "# it plots the rate of true positive to false positive along a variable threshold\n",
    "# higher rocauc score means accurate performance\n",
    "rocauc = roc_auc_score(y_test, y_pred)\n",
    "print(f'roc_auc_score : {rocauc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score : 0.9287000000000001\n"
     ]
    }
   ],
   "source": [
    "# second : cross_val_score\n",
    "clf = LogisticRegression()\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print(f'cross_val_score : {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another problem with test-train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score : 0.915\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "rocauc = roc_auc_score(y_test, y_pred)\n",
    "print(f'roc_auc_score : {rocauc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score : 0.9199999999999998\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=500)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "rocauc = roc_auc_score(y_test, y_pred)\n",
    "print(f'roc_auc_score : {rocauc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score : 0.9199999999999998\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=1000)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "rocauc = roc_auc_score(y_test, y_pred)\n",
    "print(f'roc_auc_score : {rocauc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with train test is : \n",
    "1. The Test data is not representative of the data we will see in the future.\n",
    "2. The score is biased , it is not robust as we saw the difference in accuracy \n",
    "3. we hypertune params on the cost of the train set , leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another suggestion that I didn't try here is : \n",
    "using the validation test as hyper-tuning and then testing with train-test split, this is a good idea but it is not the best. leading us back to problem no.1 and 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Nested Cross Validaiton (Robustness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nested_cross_validation](./images/nested_cross_val.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested CV Score:  0.899\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "# Inner CV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "\n",
    "# Outer CV for model evaluation\n",
    "outer_cv = KFold(n_splits=5)\n",
    "\n",
    "# Nested CV\n",
    "nested_score = cross_val_score(grid_search, X, y, cv=outer_cv)\n",
    "\n",
    "print(\"Nested CV Score: \", nested_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Mistake : Don't Train Test Split with Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A defining feature of time series data is that they're autocorrelated â€” i.e., the time series is linearly related to a lagged version of itself. (This is a fancy way of saying that observations made close together tend to be similar.)\n",
    "\n",
    "This is a problem because, if your training data set contains records which occur later than your testing data set, you're allowing your model to \"peak\" at useful information which wouldn't be available in production. We don't want our model to learn using information from the future; we want it to learn the trend using information from the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization Search \n",
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Bayesian Optimization): OrderedDict({'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2})\n",
      "Best Cross-Validation Score (Bayesian Optimization): 0.97\n",
      "Final Model Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "# Load data\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=25)\n",
    " \n",
    "# Initialize model\n",
    "model = DecisionTreeClassifier()\n",
    " \n",
    "# Define hyperparameter space for Bayesian Optimization\n",
    "param_space = {\n",
    " 'criterion': ['gini', 'entropy'],\n",
    " 'max_depth': [None] + list(range(10, 31)),\n",
    " 'min_samples_split': (2, 10),\n",
    " 'min_samples_leaf': (1, 10)\n",
    "}\n",
    "\n",
    "\n",
    "# Bayesian Optimization\n",
    "opt = BayesSearchCV(model, param_space, n_iter=32, cv=5, scoring='accuracy')\n",
    "opt.fit(X_train, y_train)\n",
    "best_params_bayes = opt.best_params_\n",
    "best_score_bayes = opt.best_score_\n",
    " \n",
    "print(f'Best Parameters (Bayesian Optimization): {best_params_bayes}')\n",
    "print(f'Best Cross-Validation Score (Bayesian Optimization): {best_score_bayes:.2f}')\n",
    "\n",
    "\n",
    "best_model = DecisionTreeClassifier(**best_params_bayes)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    " \n",
    "print(f'Final Model Accuracy: {final_accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
