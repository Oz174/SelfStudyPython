{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Process\n",
    " \n",
    "Before proceeding to generate the pipeline, we need to understand what parts of the processes can be automated. \n",
    "\n",
    "Since we want to build a process that can be used for almost any data science project, we need to first determine what steps are performed over and over again. \n",
    "\n",
    "So when working with a new data set, we usually ask the following questions:\n",
    "\n",
    "1. What format does the data come in?\n",
    "2. Does the data contain duplicates?\n",
    "3. Does the  data contain missing values?\n",
    "4. What data types does the data contain?\n",
    "5. Does the data contain outliers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What format does the data come in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data based on file extension\n",
    "def read_data(file_path):\n",
    "    _ , file_ext = os.path.splitext(file_path)\n",
    "    if file_ext == '.csv':\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_ext == '.json':\n",
    "        return pd.read_json(file_path)\n",
    "    elif file_ext in ['.xls', '.xlsx']:\n",
    "        return pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown file format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the data contain duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicates\n",
    "def drop_duplicates(df, columns=None): \n",
    "\tif columns == None: \n",
    "\t\tdf.drop_duplicates(inplace=True) \n",
    "\telse: \n",
    "\t\tdf.drop_duplicates(subset = columns, inplace=False)\n",
    "\treturn df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the data contain missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(df):\n",
    "    # Check for missing values\n",
    "    proportion_null_rows = 100*(round(df.isnull().any(axis=1).sum()/df.any(axis=1).count(),2))\n",
    "    if proportion_null_rows <= 5:\n",
    "        print(f\"There are {df.isnull().any(axis=1).sum()} rows with a null value. All of them are erased!\")\n",
    "        df.dropna()\n",
    "    else:\n",
    "        print(\"Too many null values, we need to check columns by columns further.\")\n",
    "        if df.isnull().sum().sum() > 0:\n",
    "            print(\"\\nProportion of missing values by column\")\n",
    "            values = 100*(round(df.isnull().sum()/df.count(),2))\n",
    "            print(values)\n",
    "            dealing_missing_data(df)\n",
    "        else:\n",
    "            print(\"No missing values detected!\")\n",
    "            \n",
    "\n",
    "def dealing_missing_data(df):\n",
    "    # handle the missing values\n",
    "    values = 100*(round(df.isnull().sum()/df.count(),2))\n",
    "    to_delete = []\n",
    "    to_impute = []\n",
    "    to_check = []\n",
    "    for name, proportion in values.items():\n",
    "        if int(proportion) == 0:\n",
    "            continue\n",
    "        elif int(proportion) <= 10:\n",
    "            to_impute.append(name)\n",
    "            df.fillna(df[name].median()) \n",
    "        else: \n",
    "            to_check.append(name)\n",
    "    print(f\"\\nThe missing values in {to_impute} have been replaced by the median.\")\n",
    "    print(f\"The columns {to_check} should be further understood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the data contain outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find outliers using IQR\n",
    "def find_outliers_IQR(df):\n",
    "    outlier_indices = []\n",
    "    df = df.select_dtypes(include=['number'])\n",
    "    for column in df.columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Get the indices of outliers for feature column\n",
    "        outlier_list_col = df[(df[column] < lower_bound) | (df[column] > upper_bound)].index\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "    \n",
    "    outlier_indices = list(set(outlier_indices))  # Get unique indices\n",
    "    return df.iloc[outlier_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
